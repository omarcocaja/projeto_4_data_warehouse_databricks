# Data Warehouse com Databricks e Delta Live Tables

Este projeto simula um pipeline de dados completo no Databricks, desde a ingestÃ£o de dados transacionais atÃ© a modelagem analÃ­tica em um data warehouse. A arquitetura Ã© organizada em camadas **Landing â†’ Bronze â†’ Silver â†’ Gold**, utilizando recursos como **Delta Lake**, **Delta Live Tables**, **Structured Streaming** e **Volumes do Unity Catalog**.

---

## ğŸ—‚ Estrutura do Projeto

```
projeto_4_data_warehouse_databricks/
â”‚
â”œâ”€â”€ Projeto 4/
â”‚   â”œâ”€â”€ 1 - Arquitetura/
â”‚   â”‚   â”œâ”€â”€ CRIAR_VOLUME_ADLS.py              # Script para criaÃ§Ã£o de volume no ADLS (Unity Catalog)
â”‚   â”‚   â”œâ”€â”€ Projeto 4 - DER.pdf               # Diagrama Entidade-Relacionamento (modelo dimensional)
â”‚   â”‚   â””â”€â”€ SCRIPT_DBDIAGRAM_IO               # CÃ³digo usado no dbdiagram.io
â”‚
â”‚   â”œâ”€â”€ 2 - Transacional/
â”‚   â”‚   â”œâ”€â”€ GERAR_CLIENTES.py                 # GeraÃ§Ã£o de dados simulados de clientes
â”‚   â”‚   â”œâ”€â”€ GERAR_PRODUTOS.py                 # GeraÃ§Ã£o de produtos
â”‚   â”‚   â””â”€â”€ GERAR_VENDAS.py                   # GeraÃ§Ã£o de vendas com datas e valores
â”‚
â”‚   â”œâ”€â”€ 3 - Analitico/
â”‚   â”‚   â”œâ”€â”€ 1 - landing_to_bronze/
â”‚   â”‚   â”‚   â”œâ”€â”€ DLT_LANDING_BRONZE_*.py       # DLT para ingestÃ£o batch
â”‚   â”‚   â”‚   â””â”€â”€ STREAMING_LANDING_TO_BRONZE_*.py # IngestÃ£o streaming
â”‚   â”‚   â”œâ”€â”€ 2 - bronze to silver/
â”‚   â”‚   â”‚   â””â”€â”€ BATCH_BRONZE_TO_SILVER_*.py   # TransformaÃ§Ãµes batch bronze â†’ silver
â”‚   â”‚   â””â”€â”€ 3 - silver_to_gold/
â”‚   â”‚       â””â”€â”€ GOLD_DIM_*.py                 # CriaÃ§Ã£o das dimensÃµes e fatos finais
â”‚
â”œâ”€â”€ README.md                                 # Este arquivo
â””â”€â”€ upload_this_file.zip                      # ZIP com o projeto para importar no Databricks
```

---

## âš™ï¸ Como Executar no Databricks

### 1. Crie um cluster com suporte a Delta Live Tables

- MÃ­nimo recomendado: **DBR 13.3 LTS**, **Runtime Photon**, habilitar **Unity Catalog**.

### 2. FaÃ§a o upload do projeto

- Utilize o arquivo `upload_this_file.zip` para importar todos os notebooks e scripts de uma vez.

### 3. Crie um Volume no Unity Catalog

Execute o script `CRIAR_VOLUME_ADLS.py` para criar a estrutura necessÃ¡ria no seu workspace.

### 4. Gere os dados simulados

Execute os notebooks de `2 - Transacional` para gerar os arquivos CSV na camada `landing`.

### 5. Execute os pipelines analÃ­ticos

Use os scripts de Delta Live Tables e notebooks para processar os dados atÃ© a camada gold.

---

## ğŸ“¦ Tecnologias Utilizadas

- **Databricks (Unity Catalog + Volumes + DLT)**
- **Apache Spark (Structured Streaming)**
- **Delta Lake**
- **Python 3.10+**

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a licenÃ§a **MIT** e pode ser utilizado livremente para fins educacionais e comerciais.

---

## ğŸ“¬ Contato

- [LinkedIn](https://www.linkedin.com/in/marco-caja)  
- [Instagram](https://www.instagram.com/omarcocaja)
